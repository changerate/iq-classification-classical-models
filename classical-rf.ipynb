{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6466451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sn\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import welch\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb75ff0",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a6840",
   "metadata": {},
   "source": [
    "### *variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97523cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "feature_names = [\n",
    "    \"num_iq_samples\",\n",
    "    \"amp_mean\",\n",
    "    \"amp_std\",\n",
    "    \"amp_skew\",\n",
    "    \"amp_kurtosis\",\n",
    "    \"amp_max\",\n",
    "    \"amp_min\",\n",
    "    # \"amp_25perc\",\n",
    "    # \"amp_50perc\",\n",
    "    # \"amp_75perc\",\n",
    "    \"psd_mean\",\n",
    "    \"psd_std\",\n",
    "    \"psd_max\",\n",
    "    \"psd_min\",\n",
    "    \"spectral_centroid\",\n",
    "    \"spectral_bandwidth\",\n",
    "    \"spectral_flatness\",\n",
    "    \"env_mean\",\n",
    "    \"env_std\",\n",
    "    \"freq_std\"\n",
    "]\n",
    "num_features = len(feature_names)\n",
    "max_files = 25\n",
    "\n",
    "data_dir = '/Volumes/DRIVE 128GB/iqSamples_Ruko_F11_Pro.csv'\n",
    "drone_csv_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
    "print(drone_csv_files)\n",
    "\n",
    "chunk_size = 10_000 # how many rows to hold in memory at a time\n",
    "every_n_rows = 1000\n",
    "\n",
    "# Example: sample windows with 50% overlap\n",
    "# window_size = 1024 # number of iq samples per chunk (window_size=1 is 1 sample, i,q)\n",
    "window_size = 1024 # number of iq samples per chunk (window_size=1 is 1 sample, i,q)\n",
    "window_size = 2048 # number of iq samples per chunk (window_size=1 is 1 sample, i,q)\n",
    "window_size = 4096 # number of iq samples per chunk (window_size=1 is 1 sample, i,q)\n",
    "# window_size = 8192 # number of iq samples per chunk (window_size=1 is 1 sample, i,q)\n",
    "# window_size = 16_384 # number of iq samples per chunk (window_size=1 is 1 sample, i,q)\n",
    "# window_size = 32_768 # number of iq samples per chunk (window_size=1 is 1 sample, i,q)\n",
    "\n",
    "# step = 8192\n",
    "# step = 4096\n",
    "# step = 2048\n",
    "# step = 1024\n",
    "step = 512\n",
    "\n",
    "skipSVC = False\n",
    "skipRF = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da2e6d",
   "metadata": {},
   "source": [
    "### *functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee37579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other \n",
    "def getLabelFromFilename(filename): \n",
    "    filename = filename.lower()\n",
    "    if \"phantom\" in filename: return \"Phantom\"\n",
    "    elif \"ruko_f11_pro\" in filename: return \"Ruko_F11_Pro\"\n",
    "    elif \"ruko_f11_base\" in filename: return \"Ruko_F11_base\"\n",
    "    elif \"mavic_air_2_\" in filename: return \"Mavic_Air_2\"\n",
    "    elif \"mavic_air_2s\" in filename: return \"Mavic_Air_2S\"\n",
    "    elif \"deerc\" in filename: return \"DeerC_DE2\"\n",
    "    elif \"mini_se\" in filename: return \"Mini_SE\"\n",
    "    elif \"holystone_hs100\" in filename: return \"Holystone_HS100\"\n",
    "    elif \"none\" in filename: return \"None\"\n",
    "    else: return \"unkown_label\"\n",
    "\n",
    "\n",
    "\n",
    "def load_iq(f, chunk_size, every_n_rows):\n",
    "    # The chunking in this algo is so that we can read in large files (156M lines or more). \n",
    "    # In order to reduce compution time, we skip n number of rows. \n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    for chunk in pd.read_csv(f, chunksize=chunk_size):\n",
    "        result = pd.concat([result, chunk.iloc[::every_n_rows, :]], ignore_index=True)\n",
    "\n",
    "    # print(\"THE DF:\\n\", result)\n",
    "    result = result[['i', 'q']].to_numpy()\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# metric's functions \n",
    "def checkForFilename(base_name): \n",
    "    ext = \".txt\"\n",
    "    i = 1\n",
    "    filename = f\"{base_name}-{i}{ext}\"\n",
    "    while os.path.exists(filename):\n",
    "        filename = f\"{base_name}-{i}{ext}\"\n",
    "        i += 1\n",
    "    return filename\n",
    "\n",
    "\n",
    "\n",
    "def saveMetricsToFile(base_name, perc_accuracy, model, labels): \n",
    "    filename = checkForFilename(base_name)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(f\"Model: {model}\\n\")\n",
    "        f.write(f\"Accuracy: {perc_accuracy:.2f}%\\n\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(f\"Number of samples: {len(labels)}\\n\")\n",
    "        f.write(\"Count\\tLabel\\n\")\n",
    "        counts = Counter(labels)\n",
    "        for item, count in counts.items():\n",
    "            f.write(f\"{count}\\t{item}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(f\"{cr}\\n\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        \n",
    "        f.write(f\"{cm}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ecde26",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# for numpy conversion \n",
    "\n",
    "def extractStatFeatures(iq_window, num_iq_samples): \n",
    "    \"\"\"\n",
    "    This exracts the statistical features from an iq_window number of IQ samples. \n",
    "    This creates what we're calling a \"derived\" feature. \n",
    "    \"\"\"\n",
    "    f, Pxx = welch(iq_window, nperseg=1024)  # FFT-based PSD\n",
    "    # Spectral centroid / bandwidth / flatness:\n",
    "    spectral_centroid = np.sum(f*Pxx)/np.sum(Pxx)\n",
    "    spectral_bandwidth = np.sqrt(np.sum(Pxx*(f-spectral_centroid)**2)/np.sum(Pxx))\n",
    "    spectral_flatness = np.exp(np.mean(np.log(Pxx+1e-12)))/np.mean(Pxx+1e-12)\n",
    "    # Envelope statistics: use Hilbert transform to get instantaneous amplitude\n",
    "    envelope = np.abs(hilbert(iq_window))\n",
    "    env_mean = np.mean(envelope)\n",
    "    env_std  = np.std(envelope)\n",
    "    # Instantaneous frequency variance (phase derivative)\n",
    "    phase = np.angle(iq_window)\n",
    "    inst_freq = np.diff(phase)\n",
    "    freq_std = np.std(inst_freq)\n",
    "\n",
    "    sample = [\n",
    "        int(num_iq_samples),\n",
    "        np.mean(np.abs(iq_window)),        # mean amplitude\n",
    "        np.std(np.abs(iq_window)),         # amplitude std\n",
    "        skew(np.abs(iq_window)),           # skewness\n",
    "        kurtosis(np.abs(iq_window)),       # kurtosis\n",
    "        np.max(np.abs(iq_window)),         # max\n",
    "        np.min(np.abs(iq_window)),         # min\n",
    "        # np.percentile(np.abs(iq_window),25),\n",
    "        # np.percentile(np.abs(iq_window),50),\n",
    "        # np.percentile(np.abs(iq_window),75),\n",
    "        np.mean(Pxx), np.std(Pxx), np.max(Pxx), np.min(Pxx),      # PSD stats\n",
    "        spectral_centroid, # spectral\n",
    "        spectral_bandwidth, # spectral\n",
    "        spectral_flatness, # spectral\n",
    "        env_mean, # envelope / freq\n",
    "        env_std, # envelope / freq\n",
    "        freq_std # envelope / freq\n",
    "    ]\n",
    "    \n",
    "    return np.array(sample)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def binaryIQToNumpy(meta_file):\n",
    "    data_file = meta_file.replace(\"meta\", \"data\")\n",
    "    label = getLabelFromFilename(meta_file)\n",
    "\n",
    "    try:\n",
    "        with open(meta_file, 'r') as f:\n",
    "            meta = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: {json.JSONDecodeError}\")\n",
    "        return\n",
    "\n",
    "    dtype_map = {\n",
    "        \"ri8_le\":  np.int8,\n",
    "        \"ri16_le\": np.int16,\n",
    "        \"ri32_le\": np.int32,\n",
    "        \"rf32_le\": np.float32,\n",
    "        \"cf32_le\": np.complex64,\n",
    "        \"ci8_le\":  np.int8,\n",
    "        \"ci16_le\": np.int16,\n",
    "        \"ci32_le\": np.int32,\n",
    "    }\n",
    "    dtype = dtype_map.get(meta[\"global\"][\"core:datatype\"], np.int16)\n",
    "\n",
    "    iq = np.fromfile(data_file, dtype=dtype)\n",
    "    print(f\"Converted {(len(iq) // 2):,} IQ samples to numpy array\")\n",
    "    return (iq, meta)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def readInBinDirectoryWithAnnotations(dir, postfix, max_files=100_000): \n",
    "    labels = []\n",
    "    binary_files = glob.glob(os.path.join(dir, \"**\", \"*.sigmf-meta\"), recursive=True)\n",
    "    iq_files = []\n",
    "    sigmf_meta_files = []\n",
    "    file_iter = 0\n",
    "\n",
    "    # first convert the files into a numpy array \n",
    "    for file in binary_files:\n",
    "        if file_iter < max_files: \n",
    "            iq_file, sigmf_meta = binaryIQToNumpy(file)\n",
    "            iq_files.append(iq_file)\n",
    "            sigmf_meta_files.append(sigmf_meta)\n",
    "            file_iter += 1\n",
    "    print(f\"Converted {len(sigmf_meta_files)} files\")\n",
    "    \n",
    "    # Then create a numpy array of the statistical features from the files\n",
    "    derived_samples, labels = grabStudioLabelsFromIQFiles(iq_files, sigmf_meta_files, labels=labels)\n",
    "    np.save(\"derived_samples\" + postfix + \".npy\", derived_samples)\n",
    "    np.save(\"labels_\" + postfix + \".npy\", labels)\n",
    "    return (derived_samples, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grabStudioLabelsFromIQFiles(iq_files, sigmf_meta_files, labels, max_workers=8):\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"\\nNow extracting the features\\n\")\n",
    "\n",
    "    segments = []\n",
    "\n",
    "    # Collect all segments first\n",
    "    for iq_file, meta_file in zip(iq_files, sigmf_meta_files):\n",
    "        annotations = meta_file[\"annotations\"]\n",
    "        end = -1\n",
    "\n",
    "        for ann in annotations:\n",
    "            # Non-annotated region\n",
    "            start = end + 1\n",
    "            end = ann[\"core:sample_start\"] * 2 - 1\n",
    "            if end > start:\n",
    "                labels.append(\"unknown\")\n",
    "                segments.append((iq_file[start:end].copy(), (end - start) // 2))\n",
    "                # print(f\"IQ Samples in seg: {(end - start) // 2}\")\n",
    "\n",
    "            # Annotated region\n",
    "            start = ann[\"core:sample_start\"] * 2\n",
    "            end = start + ann[\"core:sample_count\"] * 2\n",
    "            labels.append(ann[\"core:label\"])\n",
    "            segments.append((iq_file[start:end].copy(), (end - start) // 2))\n",
    "            # print(f\"IQ Samples in seg: {(end - start) // 2}\")\n",
    "\n",
    "    print(f\"Total segments to process: {len(segments):,}\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # Parallel feature extractionw\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(extractStatFeatures, seg[0], seg[1]) for seg in segments]\n",
    "        derived_samples_list = [f.result() for f in futures]  # preserves order\n",
    "\n",
    "    return np.vstack(derived_samples_list), labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2106451",
   "metadata": {},
   "source": [
    "## Loading Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4306e96",
   "metadata": {},
   "source": [
    "### *Reading in files*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bdfd1d",
   "metadata": {},
   "source": [
    "typically this has taken about 70 minutes for 45 files - 75,000 segments \n",
    "\n",
    "-11/10/2025 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6b722",
   "metadata": {},
   "source": [
    "for 30 files, 66,398 segments it took 63 mins -11/10/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 15,625,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 15,625,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 15,625,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 15,625,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 15,625,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 15,625,000 IQ samples to numpy array\n",
      "Converted 15,625,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 15,625,000 IQ samples to numpy array\n",
      "Converted 15,625,000 IQ samples to numpy array\n",
      "Converted 15,625,000 IQ samples to numpy array\n",
      "Converted 156,250,000 IQ samples to numpy array\n",
      "Converted 15,625,000 IQ samples to numpy array\n",
      "Converted 25 files\n",
      "--------------------------------------------\n",
      "\n",
      "Now extracting the features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/carlos_1/Documents/GitHub/RFML-Code/RFML_Combined_Dataset_2025/RFML_Drone_Dataset_2025/old_drone_full_annotated_dataset/RFML_Old_Drone_Training_Dataset/*'\n",
    "derived_samples_training, labels_training = readInBinDirectoryWithAnnotations(data_dir, max_files=max_files, postfix=\"_training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc5667",
   "metadata": {},
   "source": [
    "### *displaying data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Type of derived samples: {type(derived_samples_training)}\")\n",
    "print(f\"Shape of derived samples: {derived_samples_training.shape}\")\n",
    "print()\n",
    "print(f\"Type of each sample: {type(derived_samples_training[0])}\")\n",
    "print(f\"Number of features in a sample: {derived_samples_training.shape[1]}\")\n",
    "print()\n",
    "print(f\"Type of each feature: {type(derived_samples_training[0][0])}\")\n",
    "print()\n",
    "print(f\"Number of labels: {len(labels_training)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Num IQ's\\tLabel\")\n",
    "for i in range(5): \n",
    "    print(f\"{int(derived_samples_training[i][0]):,}\\t\\t{labels_training[i]}\")\n",
    "print()\n",
    "print(f\"Two samples: \\n{derived_samples_training[:2]}\")\n",
    "# print(f\"{derived_samples_training[3][0]}\\t\\t{labels_training[3]}\")\n",
    "\n",
    "print()\n",
    "print(f\"training Found:\")\n",
    "print(\"Count\\tLabel\")\n",
    "counts = Counter(labels_training)\n",
    "for item, count in counts.items():\n",
    "    print(f\"{count}\\t{item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table \n",
    "\n",
    "if False: \n",
    "    print(\"  \", end=\"\")\n",
    "    print(\" \".join(f\"{x:>29}\" for x in feature_names))\n",
    "\n",
    "    np.set_printoptions(threshold=np.inf, linewidth=1000, suppress=True)\n",
    "    print(np.array2string(\n",
    "        derived_samples,\n",
    "        formatter={'float_kind': '                     {:8.0f}'.format},\n",
    "        max_line_width=12000  # increase line width\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ee7df",
   "metadata": {},
   "source": [
    "## Random Forest Classifer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4221c0",
   "metadata": {},
   "source": [
    "### *preprocessing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9628a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    derived_samples_training = np.load('derived_samples_training.npy', allow_pickle=True)\n",
    "\n",
    "X_train = derived_samples_training\n",
    "y_train = np.array(labels_training)\n",
    "\n",
    "print(f\"Number of samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Number of labels: {len(y_train):,}\")\n",
    "\n",
    "remove_labels_training = [\n",
    "    'Burst',\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "mask = ~np.isin(y_train, remove_labels_training)\n",
    "\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "print(f\"\\nAfter removing unnecessary labels:\")\n",
    "print(f\"Number of samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Number of labels: {len(y_train):,}\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0e803",
   "metadata": {},
   "source": [
    "### *Fitting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, random_state=123)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2b038",
   "metadata": {},
   "source": [
    "### *Loading testing dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6054e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/carlos_1/Documents/GitHub/RFML-Code/RFML_Combined_Dataset_2025/RFML_Drone_Dataset_2025/old_drone_full_annotated_dataset/RFML_Old_Drone_Eval_data/*'\n",
    "derived_samples_test, labels_test = readInBinDirectoryWithAnnotations(data_dir, \"_eval\", max_files=max_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab60452",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    derived_samples_test = np.load('derived_samples_test.npy', allow_pickle=True)\n",
    "\n",
    "print(f\"Type of derived samples: {type(derived_samples_test)}\")\n",
    "print(f\"Shape of derived samples: {derived_samples_test.shape}\")\n",
    "print()\n",
    "print(f\"Type of each sample: {type(derived_samples_test[0])}\")\n",
    "print(f\"Number of features in a sample: {derived_samples_test.shape[1]}\")\n",
    "print()\n",
    "print(f\"Type of each feature: {type(derived_samples_test[0][0])}\")\n",
    "print()\n",
    "print(f\"Number of labels: {len(labels_test)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Num IQ's\\tLabel\")\n",
    "for i in range(5): \n",
    "    print(f\"{int(derived_samples_test[i][0]):,}\\t\\t{labels_test[i]}\")\n",
    "print()\n",
    "print(f\"Two samples: \\n{derived_samples_test[:2]}\")\n",
    "# print(f\"{derived_samples_test[3][0]}\\t\\t{labels_test[3]}\")\n",
    "\n",
    "print()\n",
    "print(f\"Eval Found:\")\n",
    "print(\"Count\\tLabel\")\n",
    "counts = Counter(labels_test)\n",
    "for item, count in counts.items():\n",
    "    print(f\"{count}\\t{item}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e16a51f",
   "metadata": {},
   "source": [
    "### *Prediction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5092b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = derived_samples_test\n",
    "y_test = np.array(labels_test)\n",
    "\n",
    "print(f\"Number of samples: {X_test.shape[0]:,}\")\n",
    "print(f\"Number of labels: {len(y_test):,}\")\n",
    "\n",
    "remove_labels = [\n",
    "    'Ruko_F11_pro_DL',\n",
    "    'Lightbridge_phantom_3_pro_UL',\n",
    "    'Lightbridge_phantom_3_pro_DL',\n",
    "    'Lightbridge_phantom_3_pro'\n",
    "]\n",
    "\n",
    "mask = ~np.isin(y_test, remove_labels)\n",
    "\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print(f\"\\nAfter removing invalid labels\")\n",
    "print(f\"Number of samples: {X_test.shape[0]:,}\")\n",
    "print(f\"Number of labels: {len(y_test):,}\")\n",
    "\n",
    "y_test = le.transform(y_test)\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347ac4a",
   "metadata": {},
   "source": [
    "### *Metrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90eafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "perc_accuracy = accuracy * 100\n",
    "print(f\"Accuracy: {perc_accuracy:.2f}%\")\n",
    "print()\n",
    "print(f\"Number of samples: {len(y_test)}\")\n",
    "print(\"Count\\tLabel\")\n",
    "counts = Counter(y_test)\n",
    "for item, count in counts.items():\n",
    "    print(f\"{count}\\t{item}\")\n",
    "print()\n",
    "\n",
    "cr = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cr)\n",
    "print()\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "saveMetricsToFile('rf_metrics', perc_accuracy, \"Random Forest\", labels_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the confusion matrix\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "sn.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(labels_training), yticklabels=np.unique(labels_training))\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix Heatmap for SVC Predictions')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff76e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.Series(rf.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "print(fi)\n",
    "fi.plot(kind='bar', figsize=(12,5), title=\"Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_test, y_pred))\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "print(\"MCC:\", matthews_corrcoef(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
